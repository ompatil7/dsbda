{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3531bc9c-a457-4b32-8fe0-b50375615f33",
   "metadata": {},
   "source": [
    "Text Analytics\n",
    "1. Extract sample document and apply following document preprocessing methods:\n",
    "       Tokenization,\n",
    "       POS Tagging,\n",
    "       Stop-words removal,\n",
    "       Stemming and Lemmatization,\n",
    "2. Create representation of the document by calculating Term Frequence and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b96f711-9e9d-47f1-8bf4-3252877f5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90300e3-f332-4e8d-ab8b-9a2c9b5d7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Text analytics is the process of deriving insights from text data. It involves various techniques such as tokenization, POS tagging, stop words removal, stemming, and lemmatization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "985bdb10-f80f-4253-85ca-e9b3d9c8ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ompat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de91d6-0343-480b-a139-ad5ed7166ce5",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5dd5fbce-d683-4e6c-97cd-5e0dbd662360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'insights', 'from', 'text', 'data', '.', 'It', 'involves', 'various', 'techniques', 'such', 'as', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(document)\n",
    "print(tokens)\n",
    "# array of words of the document is stored in tokens\n",
    "# if we use split instead of tokenize it will store data. and not seperate so not proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cb34b-7682-4561-80c9-c7afd0c26bec",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cd9c810-c409-425e-b837-abcab87d9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ompat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "575d20c0-310e-46ad-9df2-c83b38944bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Text', 'NN'), ('analytics', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('deriving', 'VBG'), ('insights', 'NNS'), ('from', 'IN'), ('text', 'NN'), ('data', 'NNS'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('various', 'JJ'), ('techniques', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "\n",
    "# NN - Noun, singular or mass: Examples include \"car\", \"dog\", \"city\".\n",
    "# NNS - Noun, plural: Examples include \"cars\", \"dogs\", \"cities\".\n",
    "# VBZ - Verb, 3rd person singular present: Examples include \"runs\", \"eats\", \"thinks\".\n",
    "# DT - Determiner: Examples include \"the\", \"a\", \"an\".\n",
    "# IN - Preposition or subordinating conjunction: Examples include \"in\", \"on\", \"at\", \"of\".\n",
    "# VBG - Verb, gerund or present participle: Examples include \"running\", \"eating\", \"thinking\".\n",
    "# PRP - Personal pronoun: Examples include \"I\", \"you\", \"he\", \"she\", \"it\".\n",
    "# JJ - Adjective: Examples include \"big\", \"red\", \"tall\".\n",
    "# CC - Coordinating conjunction: Examples include \"and\", \"but\", \"or\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388a2ae-c4de-41f2-9d18-f922624c267c",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0d1c40e-6285-4c2b-9f59-2f109bd33848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ompat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data for stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29dcf9bd-62ed-4053-bd17-d5112caddb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words \n",
      " {'more', 'my', 'them', 'in', 'off', 'into', 'haven', 'no', \"you'll\", 'against', 's', 'which', 'as', 'above', 'aren', 'weren', 'this', 'on', \"won't\", 'because', 'herself', 'too', 'was', \"should've\", 'between', 'out', 'a', 'don', 'where', \"weren't\", 'whom', 'so', 'not', 'why', 'any', \"mustn't\", \"it's\", 'the', 'yours', 'until', 'once', 'under', 'm', 'than', 'being', 'll', 'has', 'his', 'there', 'o', 'who', 'but', \"needn't\", 'nor', \"hasn't\", 'are', 'can', 'it', 'just', 'other', 'ma', 'won', \"you're\", 'by', 'same', 'did', \"that'll\", 'having', 'your', 'is', 'you', 'most', 'couldn', 'theirs', 'ours', 't', 'him', \"couldn't\", 'each', 'up', 'how', 'do', 'that', 'while', 'through', 're', \"aren't\", 'only', 'shouldn', 'isn', 'its', 'our', 'be', 'shan', 'i', 'or', 'now', 'had', 'for', 'again', 'wouldn', 'mightn', 'before', \"shan't\", 'further', \"she's\", 'myself', 'such', 'hers', 'should', 'over', 'an', 'me', 'then', \"wasn't\", 'her', 'during', 'wasn', 'he', 'himself', 'does', 'very', 'about', \"isn't\", 'and', 'below', 'itself', 'didn', 'she', \"hadn't\", 'needn', 'from', \"haven't\", 'those', 'doing', 'mustn', 'doesn', 'at', 'with', 'to', \"mightn't\", 'what', \"didn't\", 'here', 'of', 'hasn', 'they', 'both', \"you'd\", 'ourselves', 'these', 'd', 'after', 'been', 'were', \"you've\", 'have', \"shouldn't\", 'am', 'yourselves', 'down', 'we', 'themselves', \"doesn't\", 've', 'few', \"don't\", 'hadn', \"wouldn't\", 'yourself', 'will', 'when', 'if', 'y', 'some', 'own', 'ain', 'their', 'all'}\n",
      "Filtered tokens \n",
      " ['Text', 'analytics', 'process', 'deriving', 'insights', 'text', 'data', '.', 'involves', 'various', 'techniques', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# stop words removal\n",
    "stop_words = set(stopwords.words('english')) #all english stopwords stored\n",
    "print('Stop words \\n', stop_words)\n",
    "filtered_tokens = [ word for word in tokens if word.lower() not in stop_words ]\n",
    "print('Filtered tokens \\n', filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efa029-8acb-44a3-a5bf-953c1710b3e0",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "649e1465-a425-4715-a827-d8211b915916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens : \n",
      " ['text', 'analyt', 'process', 'deriv', 'insight', 'text', 'data', '.', 'involv', 'variou', 'techniqu', 'token', ',', 'po', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', '.']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print('Stemmed tokens : \\n', stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe184258-22c4-45ac-b655-470dbe2ffd2f",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "256c0e85-f01b-42df-9f7a-a69e6c7435b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ompat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efa44b29-874f-4041-bd24-a7299ca3a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization : \n",
      " ['Text', 'analytics', 'process', 'derive', 'insights', 'text', 'data', '.', 'involve', 'various', 'techniques', 'tokenization', ',', 'POS', 'tag', ',', 'stop', 'word', 'removal', ',', 'stem', ',', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in filtered_tokens]\n",
    "print('Lemmatization : \\n', lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb698217-de2a-4181-bc79-c6709612922c",
   "metadata": {},
   "source": [
    "#  **Task 2 : TF and IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc32b0f-c482-48e2-874c-0e1691f9f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of documents (one document in our case)\n",
    "documents = [document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11d3f30-34d6-4601-bbf9-96a8b1334e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tfidf vecotrizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b623e5a0-edb8-47a3-8998-3c2c91a23454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t0.1889822365046136\n",
      "  (0, 1)\t0.1889822365046136\n",
      "  (0, 15)\t0.1889822365046136\n",
      "  (0, 14)\t0.1889822365046136\n",
      "  (0, 24)\t0.1889822365046136\n",
      "  (0, 16)\t0.1889822365046136\n",
      "  (0, 18)\t0.1889822365046136\n",
      "  (0, 12)\t0.1889822365046136\n",
      "  (0, 22)\t0.1889822365046136\n",
      "  (0, 2)\t0.1889822365046136\n",
      "  (0, 17)\t0.1889822365046136\n",
      "  (0, 19)\t0.1889822365046136\n",
      "  (0, 23)\t0.1889822365046136\n",
      "  (0, 7)\t0.1889822365046136\n",
      "  (0, 9)\t0.1889822365046136\n",
      "  (0, 3)\t0.1889822365046136\n",
      "  (0, 5)\t0.1889822365046136\n",
      "  (0, 6)\t0.1889822365046136\n",
      "  (0, 4)\t0.1889822365046136\n",
      "  (0, 11)\t0.1889822365046136\n",
      "  (0, 13)\t0.1889822365046136\n",
      "  (0, 21)\t0.1889822365046136\n",
      "  (0, 8)\t0.1889822365046136\n",
      "  (0, 0)\t0.1889822365046136\n",
      "  (0, 20)\t0.3779644730092272\n"
     ]
    }
   ],
   "source": [
    "# fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e9997a-9524-4803-bfbd-fa78f0e8099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a995807-2d18-4a03-9678-4dd9291aa608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization : 0.1889822365046136\n",
      "and : 0.1889822365046136\n",
      "stemming : 0.1889822365046136\n",
      "removal : 0.1889822365046136\n",
      "words : 0.1889822365046136\n",
      "stop : 0.1889822365046136\n",
      "tagging : 0.1889822365046136\n",
      "pos : 0.1889822365046136\n",
      "tokenization : 0.1889822365046136\n",
      "as : 0.1889822365046136\n",
      "such : 0.1889822365046136\n",
      "techniques : 0.1889822365046136\n",
      "various : 0.1889822365046136\n",
      "involves : 0.1889822365046136\n",
      "it : 0.1889822365046136\n",
      "data : 0.1889822365046136\n",
      "from : 0.1889822365046136\n",
      "insights : 0.1889822365046136\n",
      "deriving : 0.1889822365046136\n",
      "of : 0.1889822365046136\n",
      "process : 0.1889822365046136\n",
      "the : 0.1889822365046136\n",
      "is : 0.1889822365046136\n",
      "analytics : 0.1889822365046136\n",
      "text : 0.3779644730092272\n"
     ]
    }
   ],
   "source": [
    "# print the TF-IDF representation\n",
    "for col in tfidf_matrix.nonzero()[1]:\n",
    "    print(f\"{feature_names[col]} : {tfidf_matrix[0, col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99763e20-11f4-400b-ad01-1007cb95b3d9",
   "metadata": {},
   "source": [
    "This code snippet prints the TF-IDF (Term Frequency-Inverse Document Frequency) representation of a document. TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus).\n",
    "\n",
    "Here's a breakdown of what the output means:\n",
    "\n",
    "Each line corresponds to a word (or feature) from the document.\n",
    "The number after the colon (:) is the TF-IDF score of that word in the document.\n",
    "TF-IDF is calculated based on the frequency of the word in the document (TF) and the rarity of the word in the corpus (IDF). A higher TF-IDF score indicates that the word is more important in the document.\n",
    "For example, in the output you provided:\n",
    "\n",
    "The word \"text\" has a TF-IDF score of 0.3779, which suggests it is relatively more important in the document compared to other words.\n",
    "Words like \"lemmatization\", \"and\", \"stemming\", etc., have a TF-IDF score of 0.189, indicating they are less important in the document.\n",
    "Overall, TF-IDF is used to identify the key words or terms in a document that distinguish it from other documents in the corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4644ea70-b9fc-4f33-b759-e6cc702f4397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytics : 0.1889822365046136\n",
      "and : 0.1889822365046136\n",
      "as : 0.1889822365046136\n",
      "data : 0.1889822365046136\n",
      "deriving : 0.1889822365046136\n",
      "from : 0.1889822365046136\n",
      "insights : 0.1889822365046136\n",
      "involves : 0.1889822365046136\n",
      "is : 0.1889822365046136\n",
      "it : 0.1889822365046136\n",
      "lemmatization : 0.1889822365046136\n",
      "of : 0.1889822365046136\n",
      "pos : 0.1889822365046136\n",
      "process : 0.1889822365046136\n",
      "removal : 0.1889822365046136\n",
      "stemming : 0.1889822365046136\n",
      "stop : 0.1889822365046136\n",
      "such : 0.1889822365046136\n",
      "tagging : 0.1889822365046136\n",
      "techniques : 0.1889822365046136\n",
      "text : 0.3779644730092272\n",
      "the : 0.1889822365046136\n",
      "tokenization : 0.1889822365046136\n",
      "various : 0.1889822365046136\n",
      "words : 0.1889822365046136\n"
     ]
    }
   ],
   "source": [
    "# Print the TF-IDF representation\n",
    "for word, score in zip(feature_names, tfidf_matrix.toarray()[0]):\n",
    "    print(f\"{word} : {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79dc9a42-8523-45e9-bded-b842901bda35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization : 0.1889822365046136\n",
      "and : 0.1889822365046136\n",
      "stemming : 0.1889822365046136\n",
      "removal : 0.1889822365046136\n",
      "words : 0.1889822365046136\n",
      "stop : 0.1889822365046136\n",
      "tagging : 0.1889822365046136\n",
      "pos : 0.1889822365046136\n",
      "tokenization : 0.1889822365046136\n",
      "as : 0.1889822365046136\n",
      "such : 0.1889822365046136\n",
      "techniques : 0.1889822365046136\n",
      "various : 0.1889822365046136\n",
      "involves : 0.1889822365046136\n",
      "it : 0.1889822365046136\n",
      "data : 0.1889822365046136\n",
      "from : 0.1889822365046136\n",
      "insights : 0.1889822365046136\n",
      "deriving : 0.1889822365046136\n",
      "of : 0.1889822365046136\n",
      "process : 0.1889822365046136\n",
      "the : 0.1889822365046136\n",
      "is : 0.1889822365046136\n",
      "analytics : 0.1889822365046136\n",
      "text : 0.3779644730092272\n"
     ]
    }
   ],
   "source": [
    "for row, col in zip(*tfidf_matrix.nonzero()):\n",
    "    print(f\"{feature_names[col]} : {tfidf_matrix[row, col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a9ccb-9820-4c32-b64f-302a5a8aff71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
